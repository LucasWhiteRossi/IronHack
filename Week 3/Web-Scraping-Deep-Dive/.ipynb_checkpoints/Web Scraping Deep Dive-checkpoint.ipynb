{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Deep Dive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors and Exceptions\n",
    "\n",
    "requests throws different types of exception and errors if there is ever a network problem. All exceptions are inherited from requests.exceptions.RequestException class.\n",
    "\n",
    "### Here is a short description of the common erros you may run in to:\n",
    "\n",
    "- ConnectionError exception is thrown in case of DNS failure,refused connection or any other connection related issues.\n",
    "- Timeout is raised if a request times out.\n",
    "- TooManyRedirects is raised if a request exceeds the maximum number of predefined redirections.\n",
    "- HTTPError exception is raised for invalid HTTP responses.\n",
    "\n",
    "For a more complete list and description of the exceptions you may run in to, check out the requests documentation.\n",
    "https://buildmedia.readthedocs.org/media/pdf/requests/master/requests.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HANDLING WEB RESPONSE STATUS CODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n"
     ]
    }
   ],
   "source": [
    "r2 = requests.get('https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/forbidden')\n",
    "print(r2.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Response [301]>]\n",
      "301\n",
      "<HTML><HEAD><meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
      "<TITLE>301 Moved</TITLE></HEAD><BODY>\n",
      "<H1>301 Moved</H1>\n",
      "The document has moved\n",
      "<A HREF=\"http://www.google.com/\">here</A>.\r\n",
      "</BODY></HTML>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r3 = requests.get('http://google.com')\n",
    "print(r3.history)\n",
    "\n",
    "print(r3.history[0].status_code)\n",
    "\n",
    "print(r3.history[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request was successful\n"
     ]
    }
   ],
   "source": [
    "r = requests.get('http://google.com') # `url` has been defined before\n",
    "if r.status_code < 300:\n",
    "    print('request was successful')\n",
    "elif r.status_code >= 400 and r.status_code < 500:\n",
    "    print('request failed because the resource either does not exist or is forbidden')\n",
    "else:\n",
    "    print('request failed because the response server encountered an error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Http Error: 404 Client Error: Not Found for url: http://www.google.com/blahblah\n"
     ]
    }
   ],
   "source": [
    "url='http://www.google.com/blahblah'\n",
    "\n",
    "try:\n",
    "    r = requests.get(url,timeout=3)\n",
    "    r.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print (\"Http Error:\",errh)\n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print (\"Error Connecting:\",errc)\n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print (\"Timeout Error:\",errt)\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print (\"OOps: Something Else\",err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Redirections\n",
    "\n",
    "Redirection in HTTP means forwarding the network request to a different URL. For example, if we make a request to \"http://www.github.com\", it will redirect to \"https://github.com\" using a 301 redirect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/\n",
      "[<Response [301]>, <Response [301]>]\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "r = requests.post(\"http://www.github.com\")\n",
    "print(r.url)\n",
    "print(r.history)\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the redirection process is automatically handled by requests, so you don't need to deal with it yourself. The history property contains the list of all response objects created to complete the redirection. In our example, two Response objects were created with the 301 response code. HTTP 301 and 302 responses are used for permanent and temporary redirection, respectively.\n",
    "\n",
    "If you don't want the Requests library to automatically follow redirects, then you can disable it by passing the allow_redirects=False parameter along with the request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Timeouts\n",
    "\n",
    "Another important configuration is telling our library how to handle timeouts, or requests that take too long to return. We can configure requests to stop waiting for a network requests using the timeout parameter. By default, requests will not timeout. So, if we don't configure this property, our program may hang indefinitely, which is not the functionality you'd want in a process that keeps a user waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('http://www.google.com', timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, an exception will be thrown if the server will not respond back within 1 second (which is still aggressive for a real-world application). To get this to fail more often (for the sake of an example), you need to set the timeout limit to a much smaller value, like 0.001.\n",
    "\n",
    "The timeout can be configured for both the \"connect\" and \"read\" operations of the request using a tuple, which allows you to specify both values separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://www.google.com', timeout=(5, 14))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the \"connect\" timeout is 5 seconds and \"read\" timeout is 14 seconds. This will allow your request to fail much more quicklly if it can't connect to the resource, and if it does connect then it will give it more time to download the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSL Handling\n",
    "We can also use the Requests library to verify the HTTPS certificate of a website by passing verify=true with the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.github.com', verify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will throw an error if there is any problem with the SSL of the site. If you don't want to verity, just pass False instead of True. This parameter is set to True by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading a File\n",
    "For downloading a file using requests, we can either download it by streaming the contens or directly downloading the entire thing. The stream flag is used to indicate both behaviors.\n",
    "\n",
    "As you probably guessed, if stream is True, then requests will stream the content. If stream is False, all content will be downloaded to the memory bofore returning it to you.\n",
    "\n",
    "For streaming content, we can iterate the content chunk by chunk using the iter_content method or iterate line by line using iter_line. Either way, it will download the file part by part.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://cdn.pixabay.com/photo/2018/07/05/02/50/sun-hat-3517443_1280.jpg', stream=True)\n",
    "downloaded_file = open(\"sun-hat.jpg\", \"wb\")\n",
    "for chunk in r.iter_content(chunk_size=256):\n",
    "    if chunk:\n",
    "        downloaded_file.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will download an image from Pixabay server and save it in a local file, sun-hat.jpg.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKING ASYNCHRONOUS REQUESTS\n",
    "\n",
    "When you make massive requests to websites, it can be extremely time-consuming. To complete your request faster, you can take advantage of the async module of requests. Here's how ( you'll need to first install asyncio with pip before you can try the following code ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, requests\n",
    "\n",
    "urls = [\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/breakfast.jpg',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/forbidden',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/the-html5-breakfast-site.html'\n",
    "]\n",
    "\n",
    "async def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    futures = [loop.run_in_executor(None, requests.get, url) for url in urls]\n",
    "    for response in await asyncio.gather(*futures):\n",
    "        print(response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "This event loop is already running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b0a5d64b1fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_until_complete_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnew_task\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/asyncio/base_events.py\u001b[0m in \u001b[0;36mrun_forever\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This event loop is already running'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: This event loop is already running"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "403\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "loop.run_until_complete(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEALING WITH THROTTLING AND RATE LIMITING\n",
    "\n",
    "In modern websites especially those having massive users, throttling and/or rate limiting is often enforced so that a certain person cannot make too frequent API/web requests to the website. These approaches are not targeting at regular human users but rather search engine bots and especially hackers. With throttling, the same requester (usually judged by IP address or account) cannot make more requests than the limit allowed within a certain period of time (e.g. 10,000 requests/day). If the limit is exceeded, the requester will receive an error or simply no response. With rate limiting, a requester must control the frequency of the requests under a certain threshold (e.g. 10 requests/second). When you test your web scraping scripts, if you receive a lot of errors in your responses, it does not necessarily mean the web resources are invlaid. It may because the websites you make requests to are throttling or limiting your requests.\n",
    "\n",
    "The throttling thresholds and wait peroids differ from website to website. In order to know whether you may be throttled by exceeding the limit, you need to check out the user agreement of the website that you make requests to.\n",
    "\n",
    "To control the request rate of your scripts, you can wait a period of time after making each request by calling time.sleep(). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "403\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests, time\n",
    "\n",
    "urls = [\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/breakfast.jpg',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/forbidden',\n",
    "    'https://s3-eu-west-1.amazonaws.com/ih-materials/uploads/data-static/documents/the-html5-breakfast-site.html'\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    print(response.status_code)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links \n",
    "\n",
    "https://nordicapis.com/everything-you-need-to-know-about-api-rate-limiting/\n",
    "\n",
    "https://realpython.com/python-requests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
